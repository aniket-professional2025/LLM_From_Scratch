{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-2JLzX9Ve8-"
   },
   "source": [
    "# **Check for GPU Presence and Useability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgJDTMYeVdXM",
    "outputId": "ed55b3c1-0d5d-4d4c-c31e-5957d7532ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  6 18:10:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.43                 Driver Version: 566.43         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   49C    P0             15W /   73W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdqTf3M2EOJB"
   },
   "source": [
    "# **Importing required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Wkpk0RcWEIQG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXqeFlUyEtB9"
   },
   "source": [
    "# **Setting The Configurations (Hyperparameters for this Model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "m0c6MY2bEkL9"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'csv_file_path': r\"C:\\Users\\Webbies\\Jupyter_Notebooks\\LLM_FineTune\\Website_Data_Samples_1000.csv\",\n",
    "    'train_ratio': 0.8,\n",
    "    'd_model': 256,\n",
    "    'nhead': 4,\n",
    "    'num_encoder_layers': 2,\n",
    "    'num_decoder_layers': 2,\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.1,\n",
    "    'num_epochs': 200, # This need to be increased for better performance with early stopping parameter\n",
    "    'batch_size': 4,  # Increase batch size for more data\n",
    "    'learning_rate': 1e-4,\n",
    "    'pad_token': 2,\n",
    "    'sos_token': 0,\n",
    "    'eos_token': 1,\n",
    "    'max_seq_len': 512,\n",
    "    'model_save_path': 'AI_WebsiteDesigner_LLM.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"color:green\">Future Modifications That are Needed for this Complete Process</h1>\n",
    "\n",
    "* Set the Number of Epochs to 300 or 400 with an early stopping parameter to stop the model from overfitting. Setting the epoch number to higher is needed because currently with 200 epochs, it is observed that for 200 epochs the training loss is still decreasing. \n",
    "* Increase the number of batch size from 4 to 16 or even 32 to make the model efficient by speeding the train process\n",
    "* Increase the value of the 'max_seq_len' parameter from 512 to 1024 or more to generate long codes\n",
    "* **All this modifications will lead to greater training process and will consume more memory and GPU**\n",
    "* Currently, this model building process utilizes F1 score and BLUE score that matches tokens between generated output and actual output. It performs more efficiently than the normal accuracy score that calculates percentage match as missing a single white space or comma can yield drastic mismatch.\n",
    "\n",
    "<h3 style = \"color:green\">What are the Key Features to Improve the Performance</h3>\n",
    "\n",
    "* Increase the number of Epochs (currently 200). Make it 300 to 400\n",
    "* Increase the volume of the data\n",
    "* Increase the Batch size to 16 or 32\n",
    "* Use F1 score or BLUE as evaluation metric\n",
    "* Increase the sequence length to 1024 or more big to generate more codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Download necessary NLTK data (this only needs to be run once)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Already Found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Data Already Found\")\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading 'punkt' data for NLTK...\")\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwv7c4JTFK28"
   },
   "source": [
    "# **Use GPU if available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_QY_34FFLaY",
    "outputId": "39e63c4d-2b1a-4da2-fa3d-67506c115485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8LgPafmFYUj"
   },
   "source": [
    "# **Data Loading and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GZrpBDwCFO85"
   },
   "outputs": [],
   "source": [
    "def load_process_split_data(csv_file_path, train_ratio):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV and performs a train-test split using sklearn.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Rename columns for clarity and consistency\n",
    "    df.rename(columns={'llm_generated_idea': 'prompt', 'text': 'code'}, inplace=True)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, test_size=1-train_ratio, random_state=42)\n",
    "\n",
    "    train_data = list(zip(train_df['prompt'], train_df['code']))\n",
    "    test_data = list(zip(test_df['prompt'], test_df['code']))\n",
    "\n",
    "    prompts = df['prompt'].tolist()\n",
    "    codes = df['code'].tolist()\n",
    "\n",
    "    return train_data, test_data, prompts, codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0Z5ioMJFwTQ"
   },
   "source": [
    "# **Building the Complete Vocabulary for this Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1PcPnRtDFuzv"
   },
   "outputs": [],
   "source": [
    "def build_vocab(prompts, codes):\n",
    "    \"\"\"\n",
    "    Builds a character-level vocabulary from the prompts and codes.\n",
    "    \"\"\"\n",
    "    prompt_vocab = set(\" \".join(prompts))\n",
    "    code_vocab = set(\" \".join(codes))\n",
    "\n",
    "    prompt_char_to_idx = {char: i + 3 for i, char in enumerate(sorted(list(prompt_vocab)))}\n",
    "    code_char_to_idx = {char: i + 3 for i, char in enumerate(sorted(list(code_vocab)))}\n",
    "\n",
    "    special_tokens = {\n",
    "        '<sos>': CONFIG['sos_token'],\n",
    "        '<eos>': CONFIG['eos_token'],\n",
    "        '<pad>': CONFIG['pad_token']\n",
    "    }\n",
    "    prompt_char_to_idx.update(special_tokens)\n",
    "    code_char_to_idx.update(special_tokens)\n",
    "\n",
    "    prompt_idx_to_char = {idx: char for char, idx in prompt_char_to_idx.items()}\n",
    "    code_idx_to_char = {idx: char for char, idx in code_char_to_idx.items()}\n",
    "\n",
    "    max_prompt_len = max(len(p) for p in prompts) + 2 # +2 for <sos> and <eos>\n",
    "    max_code_len = max(len(c) for c in codes) + 2    # +2 for <sos> and <eos>\n",
    "\n",
    "    print(f\"Prompt vocab size: {len(prompt_char_to_idx)}\")\n",
    "    print(f\"Code vocab size: {len(code_char_to_idx)}\")\n",
    "    print(f\"Maximum prompt length: {max_prompt_len}\")\n",
    "    print(f\"Maximum code length: {max_code_len}\")\n",
    "\n",
    "    return (prompt_char_to_idx, prompt_idx_to_char,\n",
    "            code_char_to_idx, code_idx_to_char,\n",
    "            max_prompt_len, max_code_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG4l2YoEF5vJ"
   },
   "source": [
    "# **The Tokenization Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LejNY40EF6I5"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, char_to_idx, add_sos=True, add_eos=True):\n",
    "    \"\"\"Converts a text string into a list of token indices.\"\"\"\n",
    "    tokens = [char_to_idx[char] for char in text]\n",
    "    if add_sos:\n",
    "        tokens.insert(0, CONFIG['sos_token'])\n",
    "    if add_eos:\n",
    "        tokens.append(CONFIG['eos_token'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FQOJNabGGHu"
   },
   "source": [
    "# **Pytorch Dataset and DataLoader & The Collate Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "95nhQyexGGkU"
   },
   "outputs": [],
   "source": [
    "class NLPtoCodeDataset(Dataset):\n",
    "    def __init__(self, data, prompt_char_to_idx, code_char_to_idx):\n",
    "        self.data = data\n",
    "        self.prompt_char_to_idx = prompt_char_to_idx\n",
    "        self.code_char_to_idx = code_char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt, code = self.data[idx]\n",
    "        prompt_tensor = torch.tensor(tokenize(prompt, self.prompt_char_to_idx), dtype=torch.long)\n",
    "        code_tensor = torch.tensor(tokenize(code, self.code_char_to_idx), dtype=torch.long)\n",
    "        return prompt_tensor, code_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LUTRGkeUGMHj"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length within a batch.\n",
    "    \"\"\"\n",
    "    prompts, codes = zip(*batch)\n",
    "    prompts_padded = pad_sequence(prompts, batch_first=True, padding_value=CONFIG['pad_token'])\n",
    "    codes_padded = pad_sequence(codes, batch_first=True, padding_value=CONFIG['pad_token'])\n",
    "    return prompts_padded.to(device), codes_padded.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWFYAHCEGeTR"
   },
   "source": [
    "# **Transformer Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iz3t3U5ZGZKh"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len):  # max_len is no longer a hardcoded default\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5VhjO_jvGh7P"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead,\n",
    "                 num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 dropout, max_src_len, max_tgt_len):  # Added max_src_len and max_tgt_len\n",
    "        super().__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Instantiate PositionalEncoding with the dynamic max lengths\n",
    "        self.pos_encoder_src = PositionalEncoding(d_model, dropout, max_len=max_src_len)\n",
    "        self.pos_encoder_tgt = PositionalEncoding(d_model, dropout, max_len=max_tgt_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n",
    "        src = self.pos_encoder_src(self.src_embedding(src))\n",
    "        tgt = self.pos_encoder_tgt(self.tgt_embedding(tgt))\n",
    "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "                                  src_key_padding_mask=src_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_padding_mask)\n",
    "        return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apf5iZzjG7hk"
   },
   "source": [
    "# **Training and Evaluation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JUnxa9hrGxHK"
   },
   "outputs": [],
   "source": [
    "def create_masks(src, tgt):\n",
    "    src_padding_mask = (src == CONFIG['pad_token'])\n",
    "    tgt_padding_mask = (tgt == CONFIG['pad_token'])\n",
    "    tgt_len = tgt.size(1)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(device)\n",
    "    return src_padding_mask, tgt_padding_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OVbYklgHHBFY"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"Performs a single training loop over the dataset.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src_batch, tgt_batch in dataloader:\n",
    "        tgt_input = tgt_batch[:, :-1]\n",
    "        tgt_output = tgt_batch[:, 1:]\n",
    "\n",
    "        src_padding_mask, tgt_padding_mask, tgt_mask = create_masks(src_batch, tgt_input)\n",
    "\n",
    "        output = model(src_batch, tgt_input, src_mask=None, tgt_mask=tgt_mask,\n",
    "                       src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask)\n",
    "\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vhdbfK_PHEyK"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, prompt_char_to_idx, code_idx_to_char):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and provides a simple accuracy metric.\n",
    "    We count how many generated codes exactly match the ground truth.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(test_data)\n",
    "\n",
    "    for prompt, ground_truth_code in test_data:\n",
    "        generated_code = translate(model, prompt, prompt_char_to_idx, code_idx_to_char)\n",
    "        # Remove special tokens from ground truth for fair comparison\n",
    "        ground_truth_cleaned = ground_truth_code.replace('<sos>', '').replace('<eos>', '')\n",
    "        if generated_code.strip() == ground_truth_cleaned.strip():\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **New Evaluation Function with F1 and BLEU scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_metrics(model, test_data, prompt_char_to_idx, code_idx_to_char):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set using F1-score, Precision, Recall, and BLEU score.\n",
    "    This provides a more robust measure of partial correctness.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Transformer model.\n",
    "        test_data: The list of test prompt-code pairs.\n",
    "        prompt_char_to_idx: Dictionary mapping prompt characters to indices.\n",
    "        code_idx_to_char: Dictionary mapping code indices to characters.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing the calculated precision, recall, f1_score, and bleu_score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_ground_truth_tokens = []\n",
    "    all_generated_tokens = []\n",
    "    \n",
    "    bleu_scores = []\n",
    "    chencherry = SmoothingFunction()\n",
    "\n",
    "    # Disable gradient calculations for evaluation\n",
    "    with torch.no_grad():\n",
    "        for prompt, ground_truth_code in tqdm(test_data, desc=\"Evaluating\"):\n",
    "            # Generate code from the prompt\n",
    "            generated_code = translate(model, prompt, prompt_char_to_idx, code_idx_to_char)\n",
    "            \n",
    "            # --- Prepare data for F1-score ---\n",
    "            # Create a list of integer tokens for comparison\n",
    "            # We use an inverse mapping for generated tokens to ensure they are consistent\n",
    "            ground_truth_token_ids = [\n",
    "                code_char_to_idx.get(char, CONFIG['pad_token']) for char in ground_truth_code\n",
    "            ]\n",
    "            generated_token_ids = [\n",
    "                code_char_to_idx.get(char, CONFIG['pad_token']) for char in generated_code\n",
    "            ]\n",
    "\n",
    "            # Pad the shorter sequence to match the length of the longer one\n",
    "            max_len = max(len(ground_truth_token_ids), len(generated_token_ids))\n",
    "            ground_truth_token_ids += [CONFIG['pad_token']] * (max_len - len(ground_truth_token_ids))\n",
    "            generated_token_ids += [CONFIG['pad_token']] * (max_len - len(generated_token_ids))\n",
    "            \n",
    "            all_ground_truth_tokens.extend(ground_truth_token_ids)\n",
    "            all_generated_tokens.extend(generated_token_ids)\n",
    "\n",
    "            # --- Prepare data for BLEU score ---\n",
    "            # Tokenize using characters\n",
    "            reference_tokens = [[char for char in ground_truth_code]]\n",
    "            candidate_tokens = [char for char in generated_code]\n",
    "            \n",
    "            # Calculate BLEU score for this single sentence and add to the list\n",
    "            # We use SmoothingFunction to handle cases with no n-gram matches\n",
    "            score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=chencherry.method1)\n",
    "            bleu_scores.append(score)\n",
    "\n",
    "    # Calculate overall F1, Precision, and Recall scores for all tokens\n",
    "    # 'weighted' averaging is used to account for class imbalance\n",
    "    precision = precision_score(all_ground_truth_tokens, all_generated_tokens, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_ground_truth_tokens, all_generated_tokens, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_ground_truth_tokens, all_generated_tokens, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate the average BLEU score\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    \n",
    "    return precision, recall, f1, avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KZlfYiKVHHaa"
   },
   "outputs": [],
   "source": [
    "def translate(model, prompt, prompt_char_to_idx, code_idx_to_char):\n",
    "    \"\"\"Generates code from a given prompt using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    src_tokens = tokenize(prompt, prompt_char_to_idx)\n",
    "    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    tgt_tokens = [CONFIG['sos_token']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(CONFIG['max_seq_len']):\n",
    "            tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            _, _, tgt_mask = create_masks(src_tensor, tgt_tensor)\n",
    "            output = model(src_tensor, tgt_tensor, tgt_mask=tgt_mask)\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
    "            if next_token == CONFIG['eos_token']:\n",
    "                break\n",
    "            tgt_tokens.append(next_token)\n",
    "\n",
    "    generated_code = \"\".join([code_idx_to_char[token] for token in tgt_tokens[1:] if token != CONFIG['eos_token']])\n",
    "    return generated_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj7I-dlgHWSr"
   },
   "source": [
    "# **The Main Execution Block Starts from Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Df8nURWr4C2R"
   },
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "train_data, test_data, prompts, codes = load_process_split_data(CONFIG['csv_file_path'], CONFIG['train_ratio'])\n",
    "if train_data is None:\n",
    "    print(\"Exiting due to data loading failure.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tc2hPlmA4NL6",
    "outputId": "8802a64a-e100-4a71-adc7-9a9738de9909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt vocab size: 71\n",
      "Code vocab size: 98\n",
      "Maximum prompt length: 476\n",
      "Maximum code length: 4805\n"
     ]
    }
   ],
   "source": [
    "# 2. Vocabulary Building\n",
    "(prompt_char_to_idx, prompt_idx_to_char,\n",
    " code_char_to_idx, code_idx_to_char,\n",
    " max_prompt_len, max_code_len) = build_vocab(prompts, codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzmpgbxm4WW5",
    "outputId": "2dffcc75-a649-4e46-d57c-21711878cd2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 800\n",
      "Test data size: 200\n",
      "Number of training batches: 200\n"
     ]
    }
   ],
   "source": [
    "# 3. Separating the train and test data with their respective lengths\n",
    "train_dataset = NLPtoCodeDataset(train_data, prompt_char_to_idx, code_char_to_idx)\n",
    "test_dataset = NLPtoCodeDataset(test_data, prompt_char_to_idx, code_char_to_idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Training data size: {len(train_dataset)}\")\n",
    "print(f\"Test data size: {len(test_dataset)}\")\n",
    "print(f\"Number of training batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PaFf2FN84jKU"
   },
   "outputs": [],
   "source": [
    "# 4. Model, Optimizer, and Loss Function Initialization\n",
    "model = TransformerModel(\n",
    "    src_vocab_size=len(prompt_char_to_idx),\n",
    "    tgt_vocab_size=len(code_char_to_idx),\n",
    "    d_model=CONFIG['d_model'],\n",
    "    nhead=CONFIG['nhead'],\n",
    "    num_encoder_layers=CONFIG['num_encoder_layers'],\n",
    "    num_decoder_layers=CONFIG['num_decoder_layers'],\n",
    "    dim_feedforward=CONFIG['dim_feedforward'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    max_src_len=max_prompt_len,\n",
    "    max_tgt_len=max_code_len\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tKbOZmzW4sOy"
   },
   "outputs": [],
   "source": [
    "# 5. Setting the Optimizer and Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CONFIG['pad_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eeWGSV640RG",
    "outputId": "13b33c54-a8e4-496c-bed4-49e393c32e0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Webbies\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "C:\\Users\\Webbies\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 1.7585\n",
      "Epoch [20/200], Loss: 1.4368\n",
      "Epoch [30/200], Loss: 1.2272\n",
      "Epoch [40/200], Loss: 1.0830\n",
      "Epoch [50/200], Loss: 0.9858\n",
      "Epoch [60/200], Loss: 0.9022\n",
      "Epoch [70/200], Loss: 0.8307\n",
      "Epoch [80/200], Loss: 0.7680\n",
      "Epoch [90/200], Loss: 0.7181\n",
      "Epoch [100/200], Loss: 0.6768\n",
      "Epoch [110/200], Loss: 0.6421\n",
      "Epoch [120/200], Loss: 0.6107\n",
      "Epoch [130/200], Loss: 0.5837\n",
      "Epoch [140/200], Loss: 0.5589\n",
      "Epoch [150/200], Loss: 0.5371\n",
      "Epoch [160/200], Loss: 0.5187\n",
      "Epoch [170/200], Loss: 0.5023\n",
      "Epoch [180/200], Loss: 0.4848\n",
      "Epoch [190/200], Loss: 0.4698\n",
      "Epoch [200/200], Loss: 0.4571\n",
      "--- Training complete. ---\n"
     ]
    }
   ],
   "source": [
    "# 6. The Main Training Loop\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    avg_loss = train_one_epoch(model, train_dataloader, optimizer, criterion)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{CONFIG['num_epochs']}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"--- Training complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "eHh-kVrG5EWy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving the trained model ---\n",
      "Model saved to transformer_llm.pth\n"
     ]
    }
   ],
   "source": [
    "# 7. Save the trained model\n",
    "print(\"\\n--- Saving the trained model ---\")\n",
    "torch.save(model.state_dict(), CONFIG['model_save_path'])\n",
    "print(f\"Model saved to {CONFIG['model_save_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating model performance with F1-score and BLEU score ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55ce7d1146848b7847208de17cd1cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Precision on Test Set: 0.3055\n",
      "Model Recall on Test Set: 0.0968\n",
      "Model F1-Score on Test Set: 0.1453\n",
      "Model BLEU Score on Test Set: 0.1168\n"
     ]
    }
   ],
   "source": [
    "# 8. The Modified Evaluation Process for getting the F1 and BLUE score\n",
    "print(\"\\n--- Evaluating model performance with F1-score and BLEU score ---\")\n",
    "# Call the new evaluation function\n",
    "precision, recall, f1, bleu = evaluate_model_with_metrics(model, test_data, prompt_char_to_idx, code_idx_to_char)\n",
    "\n",
    "print(f\"Model Precision on Test Set: {precision:.4f}\")\n",
    "print(f\"Model Recall on Test Set: {recall:.4f}\")\n",
    "print(f\"Model F1-Score on Test Set: {f1:.4f}\")\n",
    "print(f\"Model BLEU Score on Test Set: {bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "C7YXloCk5Q1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Inference Example ---\n",
      "Test Prompt: 'Tech Company: A minimalist design with a large, central hero image, navigation menu on the top, and a left sidebar for information about the team, services, and contact details. The footer can have a simple layout with social media icons and copyright information.'\n",
      "--------------------------------------------------\n",
      "Generated Code: \n",
      "<html>\n",
      "<link href=\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\" rel=\"stylesheet\">\n",
      "<body class=\"bg-gray-100 font-sans leading-normal tracking-normal\">\n",
      "    <header class=\"bg-white text-center\">\n",
      "         <h1 class=\"text-4xl font-bold mb-4\">Welcome to Our Tech Company</h1>\n",
      "          <p class=\"text-lg text-gray-600\">\n",
      "                At Tech Company, we are dedicated to providing innovative and services. Our team of experts is committed to delivering the best possible services. We are co\n",
      "--------------------------------------------------\n",
      "Ground Truth Code:\n",
      "<html>\n",
      "<link href=\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\" rel=\"stylesheet\">\n",
      "<body class=\"bg-gray-100 font-sans leading-normal tracking-normal\">\n",
      "    <header class=\"bg-white p-4\">\n",
      "        <nav class=\"flex justify-between\">\n",
      "            <div>\n",
      "                <a href=\"#\" class=\"text-gray-900 hover:text-orange-500\">Home</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">About</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">Services</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">Contact</a>\n",
      "            </div>\n",
      "            <div>\n",
      "                <a href=\"#\" class=\"text-gray-900 hover:text-orange-500\">Login</a>\n",
      "            </div>\n",
      "        </nav>\n",
      "    </header>\n",
      "\n",
      "    <main class=\"flex p-4\">\n",
      "        <aside class=\"w-1/4 p-4\">\n",
      "            <h2 class=\"text-2xl text-gray-900\">About Us</h2>\n",
      "            <p class=\"mt-2 text-gray-700\">We are a team of dedicated professionals committed to providing the best services in the industry.</p>\n",
      "        </aside>\n",
      "\n",
      "        <section class=\"w-3/4 p-4\">\n",
      "            <img src=\"https://source.unsplash.com/random/800x600/?tech\" alt=\"Hero Image\" class=\"w-full\">\n",
      "            <h1 class=\"mt-4 text-4xl text-gray-900\">Welcome to Our Tech Company</h1>\n",
      "            <p class=\"mt-2 text-gray-700\">At our tech company, we specialize in providing top-notch services to our clients. Our team of experts is dedicated to delivering the best possible solutions for your business needs.</p>\n",
      "        </section>\n",
      "    </main>\n",
      "\n",
      "    <footer class=\"bg-white p-4\">\n",
      "        <div class=\"flex justify-between\">\n",
      "            <div>\n",
      "                <p class=\"text-gray-700\">Â© 2022 Tech Company. All rights reserved.</p>\n",
      "            </div>\n",
      "            <div>\n",
      "                <a href=\"#\" class=\"text-gray-900 hover:text-orange-500\">Facebook</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">Twitter</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">Instagram</a>\n",
      "            </div>\n",
      "        </div>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# 9. Final Inference Example\n",
    "print(\"\\n--- Final Inference Example ---\")\n",
    "test_prompt, ground_truth_code = test_data[0]\n",
    "generated_code = translate(model, test_prompt, prompt_char_to_idx, code_idx_to_char)\n",
    "\n",
    "print(f\"Test Prompt: '{test_prompt}'\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Generated Code: \\n{generated_code}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Ground Truth Code:\\n{ground_truth_code}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
