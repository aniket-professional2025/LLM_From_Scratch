{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-2JLzX9Ve8-"
   },
   "source": [
    "# **Check for GPU Presence and Useability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgJDTMYeVdXM",
    "outputId": "ed55b3c1-0d5d-4d4c-c31e-5957d7532ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  7 16:57:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.43                 Driver Version: 566.43         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   49C    P8              4W /   72W |     283MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1820      C   C:\\Users\\Webbies\\anaconda3\\python.exe       N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdqTf3M2EOJB"
   },
   "source": [
    "# **Importing required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wkpk0RcWEIQG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXqeFlUyEtB9"
   },
   "source": [
    "# **Setting The Configurations (Hyperparameters for this Model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m0c6MY2bEkL9"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'csv_file_path': r\"C:\\Users\\Webbies\\Jupyter_Notebooks\\LLM_FineTune\\Website_Data_Samples_1000.csv\",\n",
    "    'train_ratio': 0.8,\n",
    "    'val_ratio': 0.1, \n",
    "    'd_model': 256,\n",
    "    'nhead': 4,\n",
    "    'num_encoder_layers': 2,\n",
    "    'num_decoder_layers': 2,\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.1,\n",
    "    'num_epochs': 450, # This need to be increased for better performance with early stopping parameter\n",
    "    'batch_size': 8,  # Increase batch size for more data\n",
    "    'learning_rate': 1e-4,\n",
    "    'pad_token': 2,\n",
    "    'sos_token': 0,\n",
    "    'eos_token': 1,\n",
    "    'max_seq_len': 5500,\n",
    "    'patience': 10, # New: Number of epochs to wait for improvement\n",
    "    'min_delta': 0.001, # New: Minimum change in validation loss to be considered an improvement\n",
    "    'model_save_path': 'AI_WebsiteDesigner_LLM.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"color:green\">Future Modifications That are Needed for this Complete Process</h1>\n",
    "\n",
    "* Set the Number of Epochs to 300 or 400 with an early stopping parameter to stop the model from overfitting. Setting the epoch number to higher is needed because currently with 200 epochs, it is observed that for 200 epochs the training loss is still decreasing. \n",
    "* Increase the number of batch size from 4 to 16 or even 32 to make the model efficient by speeding the train process\n",
    "* Increase the value of the 'max_seq_len' parameter from 512 to 1024 or more to generate long codes\n",
    "* **All this modifications will lead to greater training process and will consume more memory and GPU**\n",
    "* Currently, this model building process utilizes F1 score and BLUE score that matches tokens between generated output and actual output. It performs more efficiently than the normal accuracy score that calculates percentage match as missing a single white space or comma can yield drastic mismatch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Download necessary NLTK data (this only needs to be run once)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Already Found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Data Already Found\")\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading 'punkt' data for NLTK...\")\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwv7c4JTFK28"
   },
   "source": [
    "# **Use GPU if available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_QY_34FFLaY",
    "outputId": "39e63c4d-2b1a-4da2-fa3d-67506c115485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8LgPafmFYUj"
   },
   "source": [
    "# **Data Loading and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GZrpBDwCFO85"
   },
   "outputs": [],
   "source": [
    "def load_process_split_data(csv_file_path, train_ratio, val_ratio):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV and performs a train-validation-test split.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    df.rename(columns={'llm_generated_idea': 'prompt', 'text': 'code'}, inplace=True)\n",
    "\n",
    "    # Split into train+val and test sets\n",
    "    train_val_df, test_df = train_test_split(df, test_size=1-train_ratio, random_state=42)\n",
    "\n",
    "    # Split train+val into train and validation sets\n",
    "    val_size = int(len(df) * val_ratio)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=val_size, random_state=42)\n",
    "\n",
    "    train_data = list(zip(train_df['prompt'], train_df['code']))\n",
    "    val_data = list(zip(val_df['prompt'], val_df['code']))\n",
    "    test_data = list(zip(test_df['prompt'], test_df['code']))\n",
    "\n",
    "    prompts = df['prompt'].tolist()\n",
    "    codes = df['code'].tolist()\n",
    "\n",
    "    return train_data, val_data, test_data, prompts, codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0Z5ioMJFwTQ"
   },
   "source": [
    "# **Building the Complete Vocabulary for this Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "1PcPnRtDFuzv"
   },
   "outputs": [],
   "source": [
    "def build_vocab(prompts, codes):\n",
    "    \"\"\"\n",
    "    Builds a character-level vocabulary from the prompts and codes.\n",
    "    \"\"\"\n",
    "    prompt_vocab = set(\" \".join(prompts))\n",
    "    code_vocab = set(\" \".join(codes))\n",
    "\n",
    "    prompt_char_to_idx = {char: i + 3 for i, char in enumerate(sorted(list(prompt_vocab)))}\n",
    "    code_char_to_idx = {char: i + 3 for i, char in enumerate(sorted(list(code_vocab)))}\n",
    "\n",
    "    special_tokens = {\n",
    "        '<sos>': CONFIG['sos_token'],\n",
    "        '<eos>': CONFIG['eos_token'],\n",
    "        '<pad>': CONFIG['pad_token']\n",
    "    }\n",
    "    prompt_char_to_idx.update(special_tokens)\n",
    "    code_char_to_idx.update(special_tokens)\n",
    "\n",
    "    prompt_idx_to_char = {idx: char for char, idx in prompt_char_to_idx.items()}\n",
    "    code_idx_to_char = {idx: char for char, idx in code_char_to_idx.items()}\n",
    "\n",
    "    max_prompt_len = max(len(p) for p in prompts) + 2  # +2 for <sos> and <eos>\n",
    "    max_code_len = max(len(c) for c in codes) + 2     # +2 for <sos> and <eos>\n",
    "\n",
    "    print(f\"Prompt vocab size: {len(prompt_char_to_idx)}\")\n",
    "    print(f\"Code vocab size: {len(code_char_to_idx)}\")\n",
    "    print(f\"Maximum prompt length: {max_prompt_len}\")\n",
    "    print(f\"Maximum code length: {max_code_len}\")\n",
    "\n",
    "    return (prompt_char_to_idx, prompt_idx_to_char,\n",
    "            code_char_to_idx, code_idx_to_char,\n",
    "            max_prompt_len, max_code_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG4l2YoEF5vJ"
   },
   "source": [
    "# **The Tokenization Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "LejNY40EF6I5"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, char_to_idx, add_sos=True, add_eos=True):\n",
    "    \"\"\"Converts a text string into a list of token indices.\"\"\"\n",
    "    tokens = [char_to_idx[char] for char in text]\n",
    "    if add_sos:\n",
    "        tokens.insert(0, CONFIG['sos_token'])\n",
    "    if add_eos:\n",
    "        tokens.append(CONFIG['eos_token'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FQOJNabGGHu"
   },
   "source": [
    "# **Pytorch Dataset and DataLoader & The Collate Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "95nhQyexGGkU"
   },
   "outputs": [],
   "source": [
    "class NLPtoCodeDataset(Dataset):\n",
    "    def __init__(self, data, prompt_char_to_idx, code_char_to_idx):\n",
    "        self.data = data\n",
    "        self.prompt_char_to_idx = prompt_char_to_idx\n",
    "        self.code_char_to_idx = code_char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt, code = self.data[idx]\n",
    "        prompt_tensor = torch.tensor(tokenize(prompt, self.prompt_char_to_idx), dtype=torch.long)\n",
    "        code_tensor = torch.tensor(tokenize(code, self.code_char_to_idx), dtype=torch.long)\n",
    "        return prompt_tensor, code_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "LUTRGkeUGMHj"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length within a batch.\n",
    "    \"\"\"\n",
    "    prompts, codes = zip(*batch)\n",
    "    prompts_padded = pad_sequence(prompts, batch_first=True, padding_value=CONFIG['pad_token'])\n",
    "    codes_padded = pad_sequence(codes, batch_first=True, padding_value=CONFIG['pad_token'])\n",
    "    return prompts_padded.to(device), codes_padded.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWFYAHCEGeTR"
   },
   "source": [
    "# **Transformer Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "iz3t3U5ZGZKh"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len):  # max_len is no longer a hardcoded default\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "5VhjO_jvGh7P"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead,\n",
    "                 num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "                 dropout, max_src_len, max_tgt_len):  # Added max_src_len and max_tgt_len\n",
    "        super().__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Instantiate PositionalEncoding with the dynamic max lengths\n",
    "        self.pos_encoder_src = PositionalEncoding(d_model, dropout, max_len=max_src_len)\n",
    "        self.pos_encoder_tgt = PositionalEncoding(d_model, dropout, max_len=max_tgt_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n",
    "        src = self.pos_encoder_src(self.src_embedding(src))\n",
    "        tgt = self.pos_encoder_tgt(self.tgt_embedding(tgt))\n",
    "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "                                  src_key_padding_mask=src_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_padding_mask)\n",
    "        return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apf5iZzjG7hk"
   },
   "source": [
    "# **Training Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "JUnxa9hrGxHK"
   },
   "outputs": [],
   "source": [
    "def create_masks(src, tgt):\n",
    "    src_padding_mask = (src == CONFIG['pad_token'])\n",
    "    tgt_padding_mask = (tgt == CONFIG['pad_token'])\n",
    "    tgt_len = tgt.size(1)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(device)\n",
    "    return src_padding_mask, tgt_padding_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "OVbYklgHHBFY"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"Performs a single training loop over the dataset.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src_batch, tgt_batch in dataloader:\n",
    "        tgt_input = tgt_batch[:, :-1]\n",
    "        tgt_output = tgt_batch[:, 1:]\n",
    "\n",
    "        src_padding_mask, tgt_padding_mask, tgt_mask = create_masks(src_batch, tgt_input)\n",
    "\n",
    "        output = model(src_batch, tgt_input, src_mask=None, tgt_mask=tgt_mask,\n",
    "                       src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask)\n",
    "\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defineing the Evaluation Loss Function to Calculate Evaluate Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, dataloader, criterion):\n",
    "    \"\"\"Calculates the loss on a validation or test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src_batch, tgt_batch in dataloader:\n",
    "            tgt_input = tgt_batch[:, :-1]\n",
    "            tgt_output = tgt_batch[:, 1:]\n",
    "            \n",
    "            src_padding_mask, tgt_padding_mask, tgt_mask = create_masks(src_batch, tgt_input)\n",
    "\n",
    "            output = model(src_batch, tgt_input, src_mask=None, tgt_mask=tgt_mask,\n",
    "                           src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask)\n",
    "            \n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "KZlfYiKVHHaa"
   },
   "outputs": [],
   "source": [
    "def translate(model, prompt, prompt_char_to_idx, code_idx_to_char):\n",
    "    \"\"\"Generates code from a given prompt using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    src_tokens = tokenize(prompt, prompt_char_to_idx)\n",
    "    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    tgt_tokens = [CONFIG['sos_token']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(CONFIG['max_seq_len']):\n",
    "            tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            _, _, tgt_mask = create_masks(src_tensor, tgt_tensor)\n",
    "            output = model(src_tensor, tgt_tensor, tgt_mask=tgt_mask)\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
    "            if next_token == CONFIG['eos_token']:\n",
    "                break\n",
    "            tgt_tokens.append(next_token)\n",
    "\n",
    "    generated_code = \"\".join([code_idx_to_char[token] for token in tgt_tokens[1:] if token != CONFIG['eos_token']])\n",
    "    return generated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, dataloader, prompt_char_to_idx, code_idx_to_char):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on the test set using various metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predicted_tokens = []\n",
    "    all_target_tokens = []\n",
    "    \n",
    "    bleu_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src_batch, tgt_batch in dataloader:\n",
    "            for i in range(src_batch.size(0)):\n",
    "                # Get ground truth\n",
    "                ground_truth_tokens = tgt_batch[i].cpu().numpy()\n",
    "                ground_truth_tokens = ground_truth_tokens[ground_truth_tokens != CONFIG['pad_token']]\n",
    "                \n",
    "                # Get prompt\n",
    "                prompt_tokens = src_batch[i].cpu().numpy()\n",
    "                prompt_tokens = prompt_tokens[prompt_tokens != CONFIG['pad_token']]\n",
    "                \n",
    "                # Decode prompt to string for the `translate` function\n",
    "                prompt_str = \"\".join([prompt_idx_to_char[t] for t in prompt_tokens if t not in [CONFIG['sos_token'], CONFIG['eos_token'], CONFIG['pad_token']]])\n",
    "                \n",
    "                # Generate prediction\n",
    "                generated_code_str = translate(model, prompt_str, prompt_char_to_idx, code_idx_to_char)\n",
    "                \n",
    "                # Tokenize generated code for comparison\n",
    "                predicted_tokens = tokenize(generated_code_str, code_char_to_idx, add_sos=True, add_eos=True)\n",
    "                \n",
    "                # Align the sequences for token-level metrics\n",
    "                all_predicted_tokens.extend(predicted_tokens)\n",
    "                all_target_tokens.extend(ground_truth_tokens)\n",
    "                \n",
    "                # Calculate BLEU score\n",
    "                reference = [[code_idx_to_char[t] for t in ground_truth_tokens if t not in [CONFIG['sos_token'], CONFIG['eos_token'], CONFIG['pad_token']]]]\n",
    "                candidate = list(generated_code_str)\n",
    "                if candidate:\n",
    "                    bleu_scores.append(sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1))\n",
    "\n",
    "    # Calculate token-level F1, Precision, and Recall\n",
    "    min_len = min(len(all_predicted_tokens), len(all_target_tokens))\n",
    "    f1 = f1_score(all_target_tokens[:min_len], all_predicted_tokens[:min_len], average='micro')\n",
    "    precision = precision_score(all_target_tokens[:min_len], all_predicted_tokens[:min_len], average='micro')\n",
    "    recall = recall_score(all_target_tokens[:min_len], all_predicted_tokens[:min_len], average='micro')\n",
    "\n",
    "    # Calculate average BLEU score\n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "    return avg_bleu, f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Early Stopping Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0, path='checkpoint.pth'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj7I-dlgHWSr"
   },
   "source": [
    "# **The Main Execution Block Starts from Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and split data\n",
    "train_data, val_data, test_data, prompts, codes = load_process_split_data(CONFIG['csv_file_path'], CONFIG['train_ratio'], CONFIG['val_ratio'])\n",
    "if train_data is None:\n",
    "    print(\"Exiting due to data loading failure.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt vocab size: 71\n",
      "Code vocab size: 98\n",
      "Maximum prompt length: 476\n",
      "Maximum code length: 4805\n"
     ]
    }
   ],
   "source": [
    "# 2. Build vocabulary\n",
    "(prompt_char_to_idx, prompt_idx_to_char,code_char_to_idx, code_idx_to_char,max_prompt_len, max_code_len) = build_vocab(prompts, codes)\n",
    "\n",
    "src_vocab_size = len(prompt_char_to_idx)\n",
    "tgt_vocab_size = len(code_char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize datasets and dataloaders\n",
    "train_dataset = NLPtoCodeDataset(train_data, prompt_char_to_idx, code_char_to_idx)\n",
    "val_dataset = NLPtoCodeDataset(val_data, prompt_char_to_idx, code_char_to_idx)\n",
    "test_dataset = NLPtoCodeDataset(test_data, prompt_char_to_idx, code_char_to_idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize model, optimizer, and loss function\n",
    "model = TransformerModel(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=CONFIG['d_model'],\n",
    "    nhead=CONFIG['nhead'],\n",
    "    num_encoder_layers=CONFIG['num_encoder_layers'],\n",
    "    num_decoder_layers=CONFIG['num_decoder_layers'],\n",
    "    dim_feedforward=CONFIG['dim_feedforward'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    max_src_len=max_prompt_len,\n",
    "    max_tgt_len=max_code_len\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Setting the Optimizer and The Criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CONFIG['pad_token'])\n",
    "\n",
    "# 5.2 Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=CONFIG['patience'],min_delta=CONFIG['min_delta'],path=CONFIG['model_save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 10/450: Train Loss = 1.8878, Val Loss = 1.8298\n",
      "Epoch 20/450: Train Loss = 1.5760, Val Loss = 1.4956\n",
      "Epoch 30/450: Train Loss = 1.3638, Val Loss = 1.2855\n",
      "Epoch 40/450: Train Loss = 1.2238, Val Loss = 1.1471\n",
      "Epoch 50/450: Train Loss = 1.1220, Val Loss = 1.0432\n",
      "Epoch 60/450: Train Loss = 1.0416, Val Loss = 0.9660\n",
      "Epoch 70/450: Train Loss = 0.9739, Val Loss = 0.9033\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 80/450: Train Loss = 0.9157, Val Loss = 0.8486\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 90/450: Train Loss = 0.8676, Val Loss = 0.7981\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 100/450: Train Loss = 0.8228, Val Loss = 0.7518\n",
      "Epoch 110/450: Train Loss = 0.7825, Val Loss = 0.7136\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 120/450: Train Loss = 0.7497, Val Loss = 0.6817\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 130/450: Train Loss = 0.7177, Val Loss = 0.6514\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 140/450: Train Loss = 0.6897, Val Loss = 0.6240\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 150/450: Train Loss = 0.6637, Val Loss = 0.6039\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 160/450: Train Loss = 0.6445, Val Loss = 0.5837\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 170/450: Train Loss = 0.6246, Val Loss = 0.5686\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 180/450: Train Loss = 0.6042, Val Loss = 0.5546\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 190/450: Train Loss = 0.5892, Val Loss = 0.5404\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 200/450: Train Loss = 0.5718, Val Loss = 0.5312\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 4 of 10\n",
      "Epoch 210/450: Train Loss = 0.5574, Val Loss = 0.5219\n",
      "EarlyStopping counter: 5 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 220/450: Train Loss = 0.5453, Val Loss = 0.5117\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 230/450: Train Loss = 0.5311, Val Loss = 0.5036\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 4 of 10\n",
      "Epoch 240/450: Train Loss = 0.5195, Val Loss = 0.4976\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 250/450: Train Loss = 0.5092, Val Loss = 0.4898\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 4 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 260/450: Train Loss = 0.4989, Val Loss = 0.4825\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 4 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "Epoch 270/450: Train Loss = 0.4873, Val Loss = 0.4773\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 4 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 1 of 10\n",
      "EarlyStopping counter: 2 of 10\n",
      "Epoch 280/450: Train Loss = 0.4772, Val Loss = 0.4722\n",
      "EarlyStopping counter: 3 of 10\n",
      "EarlyStopping counter: 4 of 10\n",
      "EarlyStopping counter: 5 of 10\n",
      "EarlyStopping counter: 6 of 10\n",
      "EarlyStopping counter: 7 of 10\n",
      "EarlyStopping counter: 8 of 10\n",
      "EarlyStopping counter: 9 of 10\n",
      "EarlyStopping counter: 10 of 10\n",
      "Early stopping triggered at epoch 287!\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# 6. Training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    train_loss = train_one_epoch(model, train_dataloader, optimizer, criterion)\n",
    "    val_loss = evaluate_loss(model, val_dataloader, criterion)\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['num_epochs']}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving the trained model ---\n",
      "Model saved to AI_WebsiteDesigner_LLM.pth\n"
     ]
    }
   ],
   "source": [
    "# 7. Save the Model Checkpoint path in desired folder\n",
    "print(\"\\n--- Saving the trained model ---\")\n",
    "torch.save(model.state_dict(), CONFIG['model_save_path'])\n",
    "print(f\"Model saved to {CONFIG['model_save_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Inference Example ---\n",
      "Test Prompt: 'Tech Company: A minimalist design with a large, central hero image, navigation menu on the top, and a left sidebar for information about the team, services, and contact details. The footer can have a simple layout with social media icons and copyright information.'\n",
      "--------------------------------------------------\n",
      "Generated Code: \n",
      "<html>\n",
      "<link href=\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\" rel=\"stylesheet\">\n",
      "<body class=\"bg-gray-100\">\n",
      "  <header class=\"bg-white p-4 flex justify-center\">\n",
      "    <img src=\"https://source.unsplash.com/random/100x50/?logo\" alt=\"Logo\" class=\"h-10\">\n",
      "   </header>\n",
      "\n",
      "  <nav class=\"bg-white p-4 flex justify-center\">\n",
      "    <ul class=\"flex space-x-4\">\n",
      "      <li><a href=\"#\" class=\"text-gray-800 hover:text-gray-800\">Home</a></li>\n",
      "       <li><a href=\"#\" class=\"text-gray-600 hover:text-gray-800\">About</a></li>\n",
      "        <li><a href=\"#\" class=\"text-gray-800 hover:text-gray-800\">About</a></li>\n",
      "        <li><a href=\"#\" class=\"text-gray-800 hover:text-gray-800\">Contact</a></li>\n",
      "      </ul>\n",
      "    </nav>\n",
      "   <main class=\"flex flex-col items-center justify-center h-screeen\">\n",
      "     <h1 class=\"text-3xl font-bold mb-4\">Welcome to Our Tech Company</h1>\n",
      "      <p class=\"text-lg\">\n",
      "          At our tech company, we are dedicated to providing the best possible services. Our team of experts is committed to delivering high-quality products to our technology solutions to our clients. We believe in the power of technology to transform and the neeeds of the way.\n",
      "         </p>\n",
      "            <p class=\"mb-4\">\n",
      "                At our tech company, we are dedicated to providing the best possible services. Our team of experts is committed to delivering high-quality, and we solutions and services to meeet the neeeds of our clients.\n",
      "        </p>\n",
      "         </section>\n",
      "          <section id=\"about\" class=\"py-10\">\n",
      "           <h2 class=\"text-2xl font-bold mb-4\">Contact Us</h2>\n",
      "              <p class=\"text-lg\">\n",
      "                 At Tech Company, we are dedicated to providing the best possible solutions to our clients. Our team of experts is committted to delivering high-quality products to our technology solutions to the best and services. We are a team of experts and we de reliate and delivering of the to make and we are dedicated to providing the to succceeed to meeeet your neeeds.\n",
      "          </p>\n",
      "           </div>\n",
      "           </div>\n",
      "   </main>\n",
      "</body>\n",
      "</html>\n",
      "--------------------------------------------------\n",
      "Ground Truth Code:\n",
      "<html>\n",
      "<link href=\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\" rel=\"stylesheet\">\n",
      "<body class=\"bg-gray-100 font-sans leading-normal tracking-normal\">\n",
      "    <header class=\"bg-white p-4\">\n",
      "        <nav class=\"flex justify-between\">\n",
      "            <div>\n",
      "                <a href=\"#\" class=\"text-gray-900 hover:text-orange-500\">Home</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">About</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">Services</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">Contact</a>\n",
      "            </div>\n",
      "            <div>\n",
      "                <a href=\"#\" class=\"text-gray-900 hover:text-orange-500\">Login</a>\n",
      "            </div>\n",
      "        </nav>\n",
      "    </header>\n",
      "\n",
      "    <main class=\"flex p-4\">\n",
      "        <aside class=\"w-1/4 p-4\">\n",
      "            <h2 class=\"text-2xl text-gray-900\">About Us</h2>\n",
      "            <p class=\"mt-2 text-gray-700\">We are a team of dedicated professionals committed to providing the best services in the industry.</p>\n",
      "        </aside>\n",
      "\n",
      "        <section class=\"w-3/4 p-4\">\n",
      "            <img src=\"https://source.unsplash.com/random/800x600/?tech\" alt=\"Hero Image\" class=\"w-full\">\n",
      "            <h1 class=\"mt-4 text-4xl text-gray-900\">Welcome to Our Tech Company</h1>\n",
      "            <p class=\"mt-2 text-gray-700\">At our tech company, we specialize in providing top-notch services to our clients. Our team of experts is dedicated to delivering the best possible solutions for your business needs.</p>\n",
      "        </section>\n",
      "    </main>\n",
      "\n",
      "    <footer class=\"bg-white p-4\">\n",
      "        <div class=\"flex justify-between\">\n",
      "            <div>\n",
      "                <p class=\"text-gray-700\">© 2022 Tech Company. All rights reserved.</p>\n",
      "            </div>\n",
      "            <div>\n",
      "                <a href=\"#\" class=\"text-gray-900 hover:text-orange-500\">Facebook</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">Twitter</a>\n",
      "                <a href=\"#\" class=\"ml-4 text-gray-900 hover:text-orange-500\">Instagram</a>\n",
      "            </div>\n",
      "        </div>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# 9. Generate and display a sample output\n",
    "print(\"\\n--- Final Inference Example ---\")\n",
    "test_prompt, ground_truth_code = test_data[0]\n",
    "generated_code = translate(model, test_prompt, prompt_char_to_idx, code_idx_to_char)\n",
    "\n",
    "print(f\"Test Prompt: '{test_prompt}'\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Generated Code: \\n{generated_code}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Ground Truth Code:\\n{ground_truth_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 8. Getting the Model Evaluation Scores (F1 score and BLUE)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m avg_bleu, f1, precision, recall \u001b[38;5;241m=\u001b[39m evaluate_metrics(model, test_dataloader, prompt_char_to_idx, code_idx_to_char)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Final Evaluation Results ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage BLEU Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_bleu\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[98], line 26\u001b[0m, in \u001b[0;36mevaluate_metrics\u001b[1;34m(model, dataloader, prompt_char_to_idx, code_idx_to_char)\u001b[0m\n\u001b[0;32m     23\u001b[0m prompt_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([prompt_idx_to_char[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m prompt_tokens \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msos_token\u001b[39m\u001b[38;5;124m'\u001b[39m], CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meos_token\u001b[39m\u001b[38;5;124m'\u001b[39m], CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m]]])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Generate prediction\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m generated_code_str \u001b[38;5;241m=\u001b[39m translate(model, prompt_str, prompt_char_to_idx, code_idx_to_char)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Tokenize generated code for comparison\u001b[39;00m\n\u001b[0;32m     29\u001b[0m predicted_tokens \u001b[38;5;241m=\u001b[39m tokenize(generated_code_str, code_char_to_idx, add_sos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, add_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[96], line 12\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(model, prompt, prompt_char_to_idx, code_idx_to_char)\u001b[0m\n\u001b[0;32m     10\u001b[0m tgt_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tgt_tokens, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m _, _, tgt_mask \u001b[38;5;241m=\u001b[39m create_masks(src_tensor, tgt_tensor)\n\u001b[1;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src_tensor, tgt_tensor, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask)\n\u001b[0;32m     13\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     14\u001b[0m next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[110], line 27\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\u001b[0m\n\u001b[0;32m     25\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder_src(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embedding(src))\n\u001b[0;32m     26\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder_tgt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_embedding(tgt))\n\u001b[1;32m---> 27\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(src, tgt, src_mask\u001b[38;5;241m=\u001b[39msrc_mask, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[0;32m     28\u001b[0m                           src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_padding_mask,\n\u001b[0;32m     29\u001b[0m                           tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_padding_mask)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:220\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    218\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[0;32m    219\u001b[0m                       is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal)\n\u001b[1;32m--> 220\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[0;32m    221\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    222\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[0;32m    223\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:492\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    489\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[0;32m    491\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m _get_seq_len(tgt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m--> 492\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m    495\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[0;32m    496\u001b[0m                  memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[0;32m    497\u001b[0m                  tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    498\u001b[0m                  memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[0;32m    499\u001b[0m                  tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[0;32m    500\u001b[0m                  memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:971\u001b[0m, in \u001b[0;36m_detect_is_causal_mask\u001b[1;34m(mask, is_causal, size)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;66;03m# Do not use `torch.equal` so we handle batched masks by\u001b[39;00m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# broadcasting the comparison.\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m causal_comparison\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m--> 971\u001b[0m     make_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m((mask \u001b[38;5;241m==\u001b[39m causal_comparison)\u001b[38;5;241m.\u001b[39mall())\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    973\u001b[0m     make_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 8. Getting the Model Evaluation Scores (F1 score and BLUE)\n",
    "avg_bleu, f1, precision, recall = evaluate_metrics(model, test_dataloader, prompt_char_to_idx, code_idx_to_char)\n",
    "        \n",
    "print(\"\\n--- Final Evaluation Results ---\")\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "print(f\"Token-level F1 Score: {f1:.4f}\")\n",
    "print(f\"Token-level Precision: {precision:.4f}\")\n",
    "print(f\"Token-level Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Getting The preview of the Actual HTML Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_code = ground_truth_code\n",
    "\n",
    "# Create a temporary HTML file\n",
    "file_name = \"Actual.html\"\n",
    "with open(file_name, \"w\") as f:\n",
    "    f.write(html_code)\n",
    "\n",
    "# Open the file in the default web browser\n",
    "webbrowser.open(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Getting The Preview of the Generated HTML Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_code = generated_code\n",
    "\n",
    "# Create a temporary HTML file\n",
    "file_name = \"Generated.html\"\n",
    "with open(file_name, \"w\") as f:\n",
    "    f.write(html_code)\n",
    "\n",
    "# Open the file in the default web browser\n",
    "webbrowser.open(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
